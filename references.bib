@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@online{noauthor_httpsarxivorgpdf200311539_nodate,
	title = {https://arxiv.org/pdf/2003.11539},
	url = {https://arxiv.org/pdf/2003.11539},
	urldate = {2024-07-16},
	file = {https\://arxiv.org/pdf/2003.11539:/home/louis/Zotero/storage/UTYHPSDN/2003.pdf:application/pdf},
}

@misc{tian_rethinking_2020,
	title = {Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?},
	url = {http://arxiv.org/abs/2003.11539},
	shorttitle = {Rethinking Few-Shot Image Classification},
	abstract = {The focus of recent meta-learning research has been on the development of learning algorithms that can quickly adapt to test time tasks with limited data and low computational cost. Few-shot learning is widely used as one of the standard benchmarks in meta-learning. In this work, we show that a simple baseline: learning a supervised or selfsupervised representation on the meta-training set, followed by training a linear classiﬁer on top of this representation, outperforms state-of-the-art few-shot learning methods. An additional boost can be achieved through the use of selfdistillation. This demonstrates that using a good learned embedding model can be more effective than sophisticated meta-learning algorithms. We believe that our ﬁndings motivate a rethinking of few-shot image classiﬁcation benchmarks and the associated role of meta-learning algorithms. Code is available at: http://github.com/{WangYueFt}/ rfs/.},
	number = {{arXiv}:2003.11539},
	publisher = {{arXiv}},
	author = {Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B. and Isola, Phillip},
	urldate = {2024-07-16},
	date = {2020-06-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.11539 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Tian et al. - 2020 - Rethinking Few-Shot Image Classification a Good Embedding Is All You Need.pdf:/home/louis/Zotero/storage/D49GYLVE/Tian et al. - 2020 - Rethinking Few-Shot Image Classification a Good Embedding Is All You Need.pdf:application/pdf},
}

@misc{assran_self-supervised_2023,
	title = {Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture},
	url = {http://arxiv.org/abs/2301.08243},
	doi = {10.48550/arXiv.2301.08243},
	abstract = {This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-{JEPA}), a non-generative approach for self-supervised learning from images. The idea behind I-{JEPA} is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-{JEPA} towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-{JEPA} to be highly scalable. For instance, we train a {ViT}-Huge/14 on {ImageNet} using 16 A100 {GPUs} in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.},
	number = {{arXiv}:2301.08243},
	publisher = {{arXiv}},
	author = {Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and {LeCun}, Yann and Ballas, Nicolas},
	urldate = {2024-07-16},
	date = {2023-04-13},
	eprinttype = {arxiv},
	eprint = {2301.08243 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/louis/Zotero/storage/9DZJMLAZ/Assran et al. - 2023 - Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.pdf:application/pdf;arXiv.org Snapshot:/home/louis/Zotero/storage/N4DJ7D78/2301.html:text/html},
}

@incollection{fleet_food-101_2014,
	location = {Cham},
	title = {Food-101 – Mining Discriminative Components with Random Forests},
	volume = {8694},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-319-10598-7 978-3-319-10599-4},
	url = {http://link.springer.com/10.1007/978-3-319-10599-4_29},
	abstract = {In this paper we address the problem of automatically recognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share knowledge among them. To improve eﬃciency of mining and classiﬁcation, we only consider patches that are aligned with image superpixels, which we call components. To measure the performance of our rf component mining for food recognition, we introduce a novel and challenging dataset of 101 food categories, with 101’000 images. With an average accuracy of 50.76\%, our model outperforms alternative classiﬁcation methods except for cnn, including svm classiﬁcation on Improved Fisher Vectors and existing discriminative part-mining algorithms by 11.88\% and 8.13\%, respectively. On the challenging mit-Indoor dataset, our method compares nicely to other s-o-a component-based classiﬁcation methods.},
	pages = {446--461},
	booktitle = {Computer Vision – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	urldate = {2024-07-16},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-319-10599-4_29},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Bossard et al. - 2014 - Food-101 – Mining Discriminative Components with Random Forests.pdf:/home/louis/Zotero/storage/DC5CQGDU/Bossard et al. - 2014 - Food-101 – Mining Discriminative Components with Random Forests.pdf:application/pdf},
}

@article{jiang_few-shot_2020,
	title = {Few-shot Food Recognition via Multi-view Representation Learning},
	volume = {16},
	issn = {1551-6857, 1551-6865},
	url = {https://dl.acm.org/doi/10.1145/3391624},
	doi = {10.1145/3391624},
	abstract = {This article considers the problem of few-shot learning for food recognition. Automatic food recognition can support various applications, e.g., dietary assessment and food journaling. Most existing works focus on food recognition with large numbers of labelled samples, and fail to recognize food categories with few samples. To address this problem, we propose a Multi-View Few-Shot Learning ({MVFSL}) framework to explore additional ingredient information for few-shot food recognition. Besides category-oriented deep visual features, we introduce ingredient-supervised deep network to extract ingredient-oriented features. As general and intermediate attributes of food, ingredient-oriented features are informative and complementary to category-oriented features, and thus they play an important role in improving food recognition. Particularly in few-shot food recognition, ingredient information can bridge the gap between disjoint training categories and test categories. To take advantage of ingredient information, we fuse these two kinds of features by first combining their feature maps from their respective deep networks and then convolving combined feature maps. Such convolution is further incorporated into a multi-view relation network, which is capable of comparing pairwise images to enable fine-grained feature learning. {MVFSL} is trained in an end-to-end fashion for joint optimization on two types of feature learning subnetworks and relation subnetworks. Extensive experiments on different food datasets have consistently demonstrated the advantage of {MVFSL} in multi-view feature fusion. Furthermore, we extend another two types of networks, namely, Siamese Network and Matching Network, by introducing ingredient information for few-shot food recognition. Experimental results have also shown that introducing ingredient information into these two networks can improve the performance of few-shot food recognition.},
	pages = {1--20},
	number = {3},
	journaltitle = {{ACM} Transactions on Multimedia Computing, Communications, and Applications},
	shortjournal = {{ACM} Trans. Multimedia Comput. Commun. Appl.},
	author = {Jiang, Shuqiang and Min, Weiqing and Lyu, Yongqiang and Liu, Linhu},
	urldate = {2024-07-16},
	date = {2020-08-31},
	langid = {english},
	file = {Jiang et al. - 2020 - Few-shot Food Recognition via Multi-view Representation Learning.pdf:/home/louis/Zotero/storage/BRPU85LC/Jiang et al. - 2020 - Few-shot Food Recognition via Multi-view Representation Learning.pdf:application/pdf},
}

@misc{dhillon_baseline_2020,
	title = {A Baseline for Few-Shot Image Classification},
	url = {http://arxiv.org/abs/1909.02729},
	doi = {10.48550/arXiv.1909.02729},
	abstract = {Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-{ImageNet}, Tiered-{ImageNet}, {CIFAR}-{FS} and {FC}-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the {ImageNet}-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the "hardness" of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way.},
	number = {{arXiv}:1909.02729},
	publisher = {{arXiv}},
	author = {Dhillon, Guneet S. and Chaudhari, Pratik and Ravichandran, Avinash and Soatto, Stefano},
	urldate = {2024-08-17},
	date = {2020-10-21},
	eprinttype = {arxiv},
	eprint = {1909.02729 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/louis/Zotero/storage/LFCATHPH/Dhillon et al. - 2020 - A Baseline for Few-Shot Image Classification.pdf:application/pdf;arXiv.org Snapshot:/home/louis/Zotero/storage/LK4ZWA63/1909.html:text/html},
}

@inproceedings{brigato_close_2021,
	title = {A Close Look at Deep Learning with Small Data},
	url = {https://ieeexplore.ieee.org/document/9412492/?arnumber=9412492},
	doi = {10.1109/ICPR48806.2021.9412492},
	abstract = {In this work, we perform a wide variety of experiments with different deep learning architectures on datasets of limited size. According to our study, we show that model complexity is a critical factor when only a few samples per class are available. Differently from the literature, we show that in some configurations, the state of the art can be improved using low complexity models. For instance, in problems with scarce training samples and without data augmentation, low-complexity convolutional neural networks perform comparably well or better than state-of-the-art architectures. Moreover, we show that even standard data augmentation can boost recognition performance by large margins. This result suggests the development of more complex data generation/augmentation pipelines for cases when data is limited. Finally, we show that dropout, a widely used regularization technique, maintains its role as a good regularizer even when data is scarce. Our findings are empirically validated on the sub-sampled versions of popular {CIFAR}-10, Fashion-{MNIST} and, {SVHN} benchmarks.},
	eventtitle = {2020 25th International Conference on Pattern Recognition ({ICPR})},
	pages = {2490--2497},
	booktitle = {2020 25th International Conference on Pattern Recognition ({ICPR})},
	author = {Brigato, Lorenzo and Iocchi, Luca},
	urldate = {2024-08-18},
	date = {2021-01},
	note = {{ISSN}: 1051-4651},
	keywords = {Deep learning, Training, Benchmark testing, Complexity theory, Computational modeling, Computer architecture, Pipelines},
	file = {IEEE Xplore Abstract Record:/home/louis/Zotero/storage/4SFMK7YP/9412492.html:text/html;IEEE Xplore Full Text PDF:/home/louis/Zotero/storage/INBWPHCS/Brigato and Iocchi - 2021 - A Close Look at Deep Learning with Small Data.pdf:application/pdf},
}

@misc{shwartz-ziv_what_2022,
	title = {What Do We Maximize in Self-Supervised Learning?},
	url = {http://arxiv.org/abs/2207.10081},
	abstract = {In this paper, we examine self-supervised learning methods, particularly {VICReg}, to provide an information-theoretical understanding of their construction. As a ﬁrst step, we demonstrate how information-theoretic quantities can be obtained for a deterministic network, offering a possible alternative to prior work that relies on stochastic models. This enables us to demonstrate how {VICReg} can be (re)discovered from ﬁrst principles and its assumptions about data distribution. Furthermore, we empirically demonstrate the validity of our assumptions, conﬁrming our novel understanding of {VICReg}. Finally, we believe that the derivation and insights we obtain can be generalized to many other {SSL} methods, opening new avenues for theoretical and practical understanding of {SSL} and transfer learning.},
	number = {{arXiv}:2207.10081},
	publisher = {{arXiv}},
	author = {Shwartz-Ziv, Ravid and Balestriero, Randall and {LeCun}, Yann},
	urldate = {2024-08-18},
	date = {2022-07-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2207.10081 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {PDF:/home/louis/Zotero/storage/9RX24DWX/Shwartz-Ziv et al. - 2022 - What Do We Maximize in Self-Supervised Learning.pdf:application/pdf},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {25},
	url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the {LSVRC}-2010 {ImageNet} training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	urldate = {2024-08-18},
	date = {2012},
	file = {Full Text PDF:/home/louis/Zotero/storage/RCPTUDPS/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:application/pdf},
}

@misc{tan_efficientnet_2020,
	title = {{EfficientNet}: Rethinking Model Scaling for Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1905.11946},
	shorttitle = {{EfficientNet}},
	abstract = {Convolutional Neural Networks ({ConvNets}) are commonly developed at a ﬁxed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefﬁcient. We demonstrate the effectiveness of this method on scaling up {MobileNets} and {ResNet}.},
	number = {{arXiv}:1905.11946},
	publisher = {{arXiv}},
	author = {Tan, Mingxing and Le, Quoc V.},
	urldate = {2024-08-18},
	date = {2020-09-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.11946 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {PDF:/home/louis/Zotero/storage/NZPG2R2W/Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf:application/pdf},
}

@misc{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than {VGG} nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classiﬁcation task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers.},
	number = {{arXiv}:1512.03385},
	publisher = {{arXiv}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2024-08-18},
	date = {2015-12-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/home/louis/Zotero/storage/IDKBJFJ9/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: A large-scale hierarchical image database},
	url = {https://ieeexplore.ieee.org/document/5206848/?arnumber=5206848},
	doi = {10.1109/CVPR.2009.5206848},
	shorttitle = {{ImageNet}},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “{ImageNet}”, a large-scale ontology of images built upon the backbone of the {WordNet} structure. {ImageNet} aims to populate the majority of the 80,000 synsets of {WordNet} with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of {WordNet}. This paper offers a detailed analysis of {ImageNet} in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that {ImageNet} is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of {ImageNet} through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of {ImageNet} can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	eventtitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {248--255},
	booktitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	urldate = {2024-08-18},
	date = {2009-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	file = {IEEE Xplore Abstract Record:/home/louis/Zotero/storage/9TA9CAQY/5206848.html:text/html;IEEE Xplore Full Text PDF:/home/louis/Zotero/storage/3AL5Q9UD/Deng et al. - 2009 - ImageNet A large-scale hierarchical image database.pdf:application/pdf},
}

@article{li_deep_2023,
	title = {Deep metric learning for few-shot image classification: A Review of recent developments},
	volume = {138},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320323000821},
	doi = {10.1016/j.patcog.2023.109381},
	shorttitle = {Deep metric learning for few-shot image classification},
	abstract = {Few-shot image classification is a challenging problem that aims to achieve the human level of recognition based only on a small number of training images. One main solution to few-shot image classification is deep metric learning. These methods, by classifying unseen samples according to their distances to few seen samples in an embedding space learned by powerful deep neural networks, can avoid overfitting to few training images in few-shot image classification and have achieved the state-of-the-art performance. In this paper, we provide an up-to-date review of deep metric learning methods for few-shot image classification from 2018 to 2022 and categorize them into three groups according to three stages of metric learning, namely learning feature embeddings, learning class representations, and learning distance measures. Under this taxonomy, we identify the trends of transitioning from learning task-agnostic features to task-specific features, from simple computation of prototypes to computing task-dependent prototypes or learning prototypes, from using analytical distance or similarity measures to learning similarities through convolutional or graph neural networks. Finally, we discuss the current challenges and future directions of few-shot deep metric learning from the perspectives of effectiveness, optimization and applicability, and summarize their applications to real-world computer vision tasks.},
	pages = {109381},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Li, Xiaoxu and Yang, Xiaochen and Ma, Zhanyu and Xue, Jing-Hao},
	urldate = {2024-08-18},
	date = {2023-06-01},
	keywords = {Image classification, Deep neural networks, Few-shot learning, Metric learning},
	file = {ScienceDirect Full Text PDF:/home/louis/Zotero/storage/9TXCIGYD/Li et al. - 2023 - Deep metric learning for few-shot image classification A Review of recent developments.pdf:application/pdf;ScienceDirect Snapshot:/home/louis/Zotero/storage/PECUXJQT/S0031320323000821.html:text/html},
}

@misc{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	number = {{arXiv}:1412.6980},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2024-08-18},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/louis/Zotero/storage/TWSGQ23J/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/louis/Zotero/storage/IMK7GWFS/1412.html:text/html},
}

@misc{dosovitskiy_image_2021,
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	url = {http://arxiv.org/abs/2010.11929},
	shorttitle = {An Image is Worth 16x16 Words},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on {CNNs} is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks ({ImageNet}, {CIFAR}-100, {VTAB}, etc.), Vision Transformer ({ViT}) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	number = {{arXiv}:2010.11929},
	publisher = {{arXiv}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	urldate = {2024-08-18},
	date = {2021-06-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:/home/louis/Zotero/storage/U4MSTTA9/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf:application/pdf},
}

@article{maaten_visualizing_2008,
	title = {Visualizing Data using t-{SNE}},
	volume = {9},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	abstract = {We present a new technique called "t-{SNE}" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-{SNE} is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-{SNE} can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-{SNE} on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-{SNE} are significantly better than those produced by the other techniques on almost all of the data sets.},
	pages = {2579--2605},
	number = {86},
	journaltitle = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	urldate = {2024-08-18},
	date = {2008},
	file = {Full Text PDF:/home/louis/Zotero/storage/GK3NQCBY/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf:application/pdf},
}
