\citeauthor{li_deep_2023} suggest three major approaches to few shot classification in images \cite{li_deep_2023}:
\begin{enumerate}
	\item{\textbf{Meta learning methods} train a meta-learner on various classification tasks. The idea is to learn
	how to learn and be able to adapt to new classification tasks quickly \cite{li_deep_2023}.}
	\item{\textbf{Transfer learning methods} rely on the assumption that the source and target domain are 
	somewhat related, and a model can be fine-tuned on the target domain \cite{li_deep_2023}.}
	\item{\textbf{Metric learning methods} leverage a feature embedding model and use a subsequent base learner 
	to perform the classification task based on the embeddings \cite{li_deep_2023}.}
\end{enumerate}

Metric learning methods have the advantage of not having to learn new parameters when new classes are added 
and thus overcome the problem of overfitting \cite{li_deep_2023}. \citeauthor{tian_rethinking_2020} use such a metric learning method by training a classification model on the labeled training classes, 
dropping the classification head, and replacing it by a new head trained at test time on the test classes \cite{tian_rethinking_2020}. In particular, they used logistic regression for the classification head \cite{tian_rethinking_2020}. 
\citeauthor{dhillon_baseline_2020} have a similar approach using embedding regularization and cosine similarity 
for classification on the test classes \cite{dhillon_baseline_2020}.

While \cite{tian_rethinking_2020, dhillon_baseline_2020} use one embedding model to extract meaningful feature maps, 
\cite{jiang_few-shot_2020} introduces multi-view representation learning for food classification specifically.
\citeauthor{jiang_few-shot_2020} utilize multiple convolutional networks that predict several different aspects 
of the given food image, such as food category and ingredients and subsequently fuse the individual feature maps to 
make a final class prediction.

While all the methods above use supervised learning for training the initial foundation model, this work 
explores a self-supervised learning method to learn meaningful representations of food images that can 
be used for training a base learner (e.g. logistic regression classifier) to predict the test classes.

The self-supervised learning method \gls{ijepa} is an architecture that consists of an encoder, a predictor, and a 
target encoder \cite{assran_self-supervised_2023}. Instead of relying on \glspl{cnn}, the \gls{ijepa} uses 
the \gls{vit} \cite{dosovitskiy_image_2021} as its basic building block \cite{assran_self-supervised_2023}. 
The underlying idea is to feed an image through the target encoder,
a masked version of that same image through the encoder, and predict the embedding from the original image based 
on the embedding of the masked image using the predictor \cite{assran_self-supervised_2023}. They show that 
by predicting in representation space rather than in the original image space, \gls{ijepa} is capable of 
producing semantic representations while using less training hours than other methods \cite{assran_self-supervised_2023}.
